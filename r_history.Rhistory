## In this example, the data is in a matrix called
## data.matrix
## columns are individual samples (i.e. cells)
## rows are measurements taken for all the samples (i.e. genes)
## Just for the sake of the example, here's some made up data...
data.matrix <- matrix(nrow=100, ncol=10)
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""),
paste("ko", 1:5, sep=""))
rownames(data.matrix) <- paste("gene", 1:100, sep="")
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i,] <- c(wt.values, ko.values)
}
head(data.matrix)
dim(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE)
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
#######
##
## NOTE: Everything that follow is just bonus stuff.
## It simply demonstrates how to get the same
## results using "svd()" (Singular Value Decomposition) or using "eigen()"
## (Eigen Decomposition).
##
#######
############################################
##
## Now let's do the same thing with svd()
##
## svd() returns three things
## v = the "rotation" that prcomp() returns, this is a matrix of eigenvectors
##     in other words, a matrix of loading scores
## u = this is similar to the "x" that prcomp() returns. In other words,
##     sum(the rotation * the original data), but compressed to the unit vector
##     You can spread it out by multiplying by "d"
## d = this is similar to the "sdev" value that prcomp() returns (and thus
##     related to the eigen values), but not
##     scaled by sample size in an unbiased way (ie. 1/(n-1)).
##     For prcomp(), sdev = sqrt(var) = sqrt(ss(fit)/(n-1))
##     For svd(), d = sqrt(ss(fit))
##
############################################
svd.stuff <- svd(scale(t(data.matrix), center=TRUE))
## calculate the PCs
svd.data <- data.frame(Sample=colnames(data.matrix),
X=(svd.stuff$u[,1] * svd.stuff$d[1]),
Y=(svd.stuff$u[,2] * svd.stuff$d[2]))
svd.data
## alternatively, we could compute the PCs with the eigen vectors and the
## original data
svd.pcs <- t(t(svd.stuff$v) %*% t(scale(t(data.matrix), center=TRUE)))
svd.pcs[,1:2] ## the first to principal components
svd.df <- ncol(data.matrix) - 1
svd.var <- svd.stuff$d^2 / svd.df
svd.var.per <- round(svd.var/sum(svd.var)*100, 1)
ggplot(data=svd.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", svd.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", svd.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("svd(scale(t(data.matrix), center=TRUE)")
############################################
##
## Now let's do the same thing with eigen()
##
## eigen() returns two things...
## vectors = eigen vectors (vectors of loading scores)
##           NOTE: pcs = sum(loading scores * values for sample)
## values = eigen values
##
############################################
cov.mat <- cov(scale(t(data.matrix), center=TRUE))
dim(cov.mat)
## since the covariance matrix is symmetric, we can tell eigen() to just
## work on the lower triangle with "symmetric=TRUE"
eigen.stuff <- eigen(cov.mat, symmetric=TRUE)
dim(eigen.stuff$vectors)
head(eigen.stuff$vectors[,1:2])
eigen.pcs <- t(t(eigen.stuff$vectors) %*% t(scale(t(data.matrix), center=TRUE)))
eigen.pcs[,1:2]
eigen.data <- data.frame(Sample=rownames(eigen.pcs),
X=(-1 * eigen.pcs[,1]), ## eigen() flips the X-axis in this case, so we flip it back
Y=eigen.pcs[,2]) ## X axis will be PC1, Y axis will be PC2
eigen.data
eigen.var.per <- round(eigen.stuff$values/sum(eigen.stuff$values)*100, 1)
ggplot(data=eigen.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", eigen.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", eigen.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("eigen on cov(t(data.matrix))")
knitr::opts_chunk$set(echo = TRUE)
#### Worksheet Clustering
##Select statements from the following code and the clusteringR ppt to get protein datamatrix
##generate a distance matrix, dendogram 13x13 of proteins and plot it
##by using eucledian and average linkage and then create also a heamap in an .rmd file
##then add to your rmd file the answers/plots/code for the following questions.
##your code should plot within the .html when you knit and not send the plot to a pdf
#reading the protein.csv file
prot=read.csv("protein.csv")
#scaling
scaledprot=scale(prot, center=TRUE)
#euclidean distance method
d1=dist(t(scaledprot), method="euclidean")
#average linkage method
hclust(d1,method="average")
#dendrogram plotting
plot(hclust(d1, method="average"))
#heatmap
heatmap(as.matrix(scaledprot))
##QUESTIONS TO ANSWER IN YOUR Rmarkdown (if you do not have enough time complete it at home).
##NOTE: use par() to partition your plots to compare
#QUESTION1: Pick another method to generate your cluster and compare a) topology and b) branchlengths.
#you should generate dendograms and/or heatmaps and comment on differences and similarities
#e.g."euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" these are available.
#distance method
d2=dist(t(scaledprot), method="manhattan")
#average linkage method
hclust(d2,method="average")
#dendrogram plotting
par( mfrow= c(1,2) )
plot(hclust(d1,method="average"))
plot(hclust(d2, method="average"))
#heatmap
heatmap(as.matrix(d1))
heatmap(as.matrix(d2))
#a) The topology is the same for both the Manhattan and Euclidian distance measures.
#b) As height increases in the dendrogram created using the Manhattan approach, branchlengths also increase.
#QUESTION2: Compare single linkage with average linkage and comment on topology and branchlengths?
#Use the same distance method to see the effect of linkage.
#distance method
d3=dist(t(scaledprot), method="manhattan")
#average linkage method
hclust1 = hclust(d3,method="single")
#dendrogram plotting
par( mfrow= c(1,2) )
plot(hclust(d2, method="average"))
plot(hclust(d3, method="single"))
#a) For average and single linkage methods, the topology is the same.
#b) In the dendrogram generated using the average approach, branchlengths increase as height increases.
#QUESTION3: Discuss when would you use average and when would you use single? Or should you use them both? Why or why not?
# If the data are less noisy, a single linkage should be employed (outliers). Between single and full connection, average linkage strikes a balance. In addition, it performs superior to single linkage on a noisy dataset. Selecting the best method for the given dataset can be aided by combining the two of them.
#QUESTION4: generate a heatmap of fractions in which proteins are expressed (6 x 6) instead of (13 x 13).
#Explain what did you do different to get that heatmap.
d4=dist(scaledprot, method="euclidean")
heatmap(as.matrix(d4))
# Instead of transposing the scaledprot variable inside the dist function, I used it directly to obtain the heatmap of fractions.
#QUESTION5: change the color palette of one of the heatmaps you generated to one of your choice. How did you find which pallette you can use? Explain.
library(RColorBrewer)
heatmap(as.matrix(scaledprot), col=colorRampPalette(brewer.pal(8, "Blues"))(25))
# I examined the RColorBrewer palettes and the native R palettes. I went with the one that is created from similar color shades.
#PCA
#normalization of data
rangescale <- function(X) {
Xmax = apply(X, 2, max)
Xscaled <- scale(X, scale=Xmax, center=FALSE)
return(Xscaled)
}
scaleddata <- rangescale(prot)
#a)
?prcomp
#b) Principle Component (PC) scores are generated by x, and loadings are generated by rotation.
#c) The R code used in the lecture notes and the video for visualizing the PC scores differs in that the lecture notes use rotation values whereas the movie uses x values. Using x values to depict the PC scores is the appropriate strategy since x includes the major components.
#d)
result = prcomp(scaleddata, center=FALSE)
result2 = prcomp(t(scaleddata), center=FALSE)
par( mfrow= c(1,2) )
#PC1 vs PC2 showing all fractions
scores = result$x
plot(scores[,1], scores[,2], xlab="PC1", ylab="PC2")
text(scores[,1]+0.005, scores[,2]+0.003, labels = rownames(prot), pos=4)
#PC1 vs PC2 showing all proteins
scores2 = result2$x
plot(scores2[,1], scores2[,2], xlab="PC1", ylab="PC2")
text(scores2[,1]+0.005, scores2[,2]+0.003, labels = colnames(prot), pos=4)
#e) Since the first plot provides the relationship between the fractions by creating PCs that are composed of linear combinations of fractions, and the second plot provides the relationship between the proteins by creating PCs that are composed of linear combinations of proteins, the first plot uses normal data while the second plot uses transposed data.
#f) As proteins (samples) are columns and fractions (variables) are rows, we must transpose the data to identify the relationship between the proteins and utilize the normal matrix to find the relationship between the fractions.
savehistory("~/Downloads/r_history.Rhistory")
